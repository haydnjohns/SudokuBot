"""
Train / load a CNN that maps ONE Sudoku cell (BGR or Gray) to
   0 ... 9  for a digit
   0        for an empty / unsure cell
The *only* training data used are pictures generated by sudoku_renderer
that are run through the very same extractor as at inference‑time.
"""
from __future__ import annotations
import os
from pathlib import Path
from typing import Iterable, Tuple
import random
import cv2
import numpy as np
from tqdm import tqdm
import keras
from keras import layers, models
import torch

import sudoku_renderer as sr
import digit_extractor as de

DATA_DIR = Path(__file__).with_name("data")
DATA_DIR.mkdir(exist_ok=True)
MODEL_FNAME = DATA_DIR / "cnn_digit_model.keras"
TRAINING_CACHE = DATA_DIR / "training_cache"


class DigitClassifier:
    def __init__(self,
                 *,
                 device: torch.device | str | None = None,
                 train: bool = False,
                 n_boards: int = 2000,
                 force_retrain: bool = False,
                 ):
        """
        train=True           – build a fresh training set (n_boards pictures)
        force_retrain=True   – ignore any cached model on disk
        """
        self.device = torch.device(device) if device else torch.device("cpu")

        if MODEL_FNAME.exists() and not force_retrain:
            print(f"[DigitClassifier] loading model from {MODEL_FNAME}")
            self.model = keras.saving.load_model(MODEL_FNAME)
        else:
            if not train:
                raise RuntimeError(
                    "No pre‑trained model found on disk. "
                    "Instantiate with train=True to bootstrap one."
                )
            print("[DigitClassifier] training CNN from scratch ...")
            x, y = (
                self._create_training_set(n_boards)
                if train
                else (None, None)
            )
            self.model = self._train_model(x, y)
            self.model.save(MODEL_FNAME)
            print(f"[DigitClassifier] model saved to {MODEL_FNAME}")

    # ---------------------------------------------------------------- #
    # public helpers
    # ---------------------------------------------------------------- #
    @torch.no_grad()
    def predict(self, cell_bgr: np.ndarray) -> Tuple[int, float]:
        """
        Returns  (digit, confidence).  digit==0 means 'empty / unsure'.
        """
        if cell_bgr.ndim == 3:
            cell = cv2.cvtColor(cell_bgr, cv2.COLOR_BGR2GRAY)
        else:
            cell = cell_bgr

        # adaptive binary
        thr = cv2.adaptiveThreshold(cell, 255,
                                    cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                    cv2.THRESH_BINARY_INV, 15, 4)

        # downsize to 28×28, keep aspect
        sample = cv2.resize(thr, (28, 28), interpolation=cv2.INTER_AREA)
        sample = sample.astype("float32") / 255.0
        sample = sample[None, ..., None]          # add batch+channel dims

        logits = self.model(sample, training=False)[0].numpy()
        idx = int(logits.argmax())
        conf = float(logits[idx])

        if idx == 10 or conf < 0.70:   # 10 == empty class
            return 0, conf
        return idx, conf

    # ---------------------------------------------------------------- #
    # training
    # ---------------------------------------------------------------- #
    @staticmethod
    def _prepare(imgs: Iterable[np.ndarray]) -> np.ndarray:
        """imgs are 28×28 already, returns float32 (N,28,28,1)"""
        X = np.stack(imgs).astype("float32") / 255.0
        X = X[..., None]
        return X

    def _create_training_set(self, n_boards: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        • Render n_boards synthetic photos.
        • Extract the 81 crops of each picture.
        • Cache the resulting  (X,Y)  on disk to avoid re‑rendering next run.
        """
        X_file = TRAINING_CACHE / f"X_{n_boards}.npy"
        Y_file = TRAINING_CACHE / f"Y_{n_boards}.npy"
        TRAINING_CACHE.mkdir(exist_ok=True, parents=True)

        if X_file.exists():
            print("[DigitClassifier] reusing cached training data")
            X = np.load(X_file)
            Y = np.load(Y_file)
            return X, Y

        print(f"[DigitClassifier] generating {n_boards} synthetic boards …")
        renderer = sr.SudokuRenderer()
        X_list, Y_list = [], []

        for _ in tqdm(range(n_boards)):
            img, board = renderer.render()
            cnt = de.locate_grid_outline(img)
            warped = de.warp_grid(img, cnt, side=450)
            cells = de.split_into_cells(warped)

            for cell_img, label in zip(cells, board.ravel()):
                # prepare exactly like predict()
                thr = cv2.adaptiveThreshold(cv2.cvtColor(cell_img, cv2.COLOR_BGR2GRAY),
                                            255,
                                            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                            cv2.THRESH_BINARY_INV, 15, 4)
                thr = cv2.resize(thr, (28, 28), cv2.INTER_AREA)
                X_list.append(thr)
                Y_list.append(label if label != 0 else 10)  # 10 = empty

        X = self._prepare(X_list)
        Y = np.array(Y_list, np.int64)
        np.save(X_file, X)
        np.save(Y_file, Y)
        print(f"[DigitClassifier] cached training data in {TRAINING_CACHE}")
        return X, Y

    def _train_model(self, X: np.ndarray, y: np.ndarray) -> keras.Model:
        idx = np.random.permutation(len(X))
        X, y = X[idx], y[idx]
        split = int(0.9 * len(X))
        (x_tr, y_tr), (x_va, y_va) = (X[:split], y[:split]), (X[split:], y[split:])

        data_augment = keras.Sequential(
            [
                layers.RandomRotation(0.1, fill_mode="constant", fill_value=0.0),
                layers.RandomTranslation(0.1, 0.1, fill_mode="constant", fill_value=0.0),
                layers.RandomZoom(0.1, 0.1, fill_mode="constant", fill_value=0.0),
            ]
        )

        m = models.Sequential(
            [
                layers.Input(shape=(28, 28, 1)),
                data_augment,
                layers.Rescaling(1.0 / 1.0),  # already 0–1
                layers.Conv2D(32, 3, activation="relu"),
                layers.Conv2D(32, 3, activation="relu"),
                layers.MaxPooling2D(2),
                layers.Conv2D(64, 3, activation="relu"),
                layers.MaxPooling2D(2),
                layers.Flatten(),
                layers.Dense(128, activation="relu"),
                layers.Dropout(0.4),
                layers.Dense(11, activation="softmax"),  # 0‑9 + empty
            ]
        )
        m.compile(optimizer="adam",
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])
        m.summary()
        m.fit(x_tr, y_tr,
              epochs=20,
              batch_size=256,
              validation_data=(x_va, y_va),
              callbacks=[keras.callbacks.EarlyStopping(patience=4,
                                                       restore_best_weights=True)],
              verbose=2)
        return m